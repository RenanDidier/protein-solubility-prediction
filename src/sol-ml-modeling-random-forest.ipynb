{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit Learn libraries\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Scipy libraries\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/\"\n",
    "\n",
    "data_path = folder_path + \"complex_processed_data.csv\"\n",
    "standardized_data_path = folder_path + 'complex_processed_standardized_data.csv'\n",
    "standardized_poutliers_removed_data_path = folder_path + 'complex_processed_standardized_outliers_removed_data.csv'\n",
    "\n",
    "df_solubility = pd.read_csv(standardized_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataset\n",
    "\n",
    "Process Dataset before the model creation.\n",
    "The following actions were done:\n",
    "* Split the independent variable from the dependent ones;\n",
    "* Split Dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into X and Y for machine learning\n",
    "\n",
    "df_sol_X = df_solubility.copy()\n",
    "df_sol_X.drop(columns=['solubility'], axis=1, inplace=True)\n",
    "\n",
    "df_sol_y = df_solubility[['solubility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                        df_sol_X, df_sol_y, \n",
    "                        train_size = 0.8,\n",
    "                        test_size = 0.2,\n",
    "                        random_state = 10\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting regression - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def get_adj_r2(n_observations, n_independent_variables, r2_score):\n",
    "    Adj_r2 = 1 - ((1 - r2_score) * (n_observations - 1)) / (n_observations - n_independent_variables - 1)\n",
    "    return Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validation scheme to be used for train and test\n",
    "cv = 10\n",
    "folds = KFold(n_splits = cv, shuffle = True, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r2 score on test set: 0.1369\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "\n",
    "xgboost = ensemble.GradientBoostingRegressor(**params)\n",
    "xgboost.fit(x_train, y_train)\n",
    "\n",
    "r2 = r2_score(y_test, xgboost.predict(x_test))\n",
    "print(\"The r2 score on test set: {:.4f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4800 candidates, totalling 48000 fits\n",
      "{'learning_rate': 0.01, 'loss': 'squared_error', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Specify range of hyperparameters to tune\n",
    "hyper_params = {\n",
    "    'n_estimators':[100, 200, 300, 400, 500],\n",
    "    'max_depth':[2, 3, 4,5],\n",
    "    \"min_samples_split\": [1,2,3,4],\n",
    "    \"min_samples_leaf\": [1,1.5,2],\n",
    "    \"learning_rate\": [0.01,0.02,0.03,0.4,0.5],\n",
    "    \"loss\": [\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"],\n",
    "    #\"criterion\": [\"friedman_mse\", \"squared_error\", \"mse\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Call GridSearchCV()\n",
    "model_cv = GridSearchCV(\n",
    "    estimator = ensemble.GradientBoostingRegressor(),\n",
    "    param_grid = hyper_params,\n",
    "    scoring= 'r2',\n",
    "    cv = folds,\n",
    "    verbose = 1,\n",
    "    return_train_score=True,\n",
    "    n_jobs = -1,\n",
    "    refit = True\n",
    "    )\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "best_model = model_cv.fit(x_train, np.ravel(y_train)) \n",
    "\n",
    "print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=3,\n",
       "                          n_estimators=300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new model with best_params_ from grid search\n",
    "# Use cross validation on the best_params_ model\n",
    "\n",
    "xgboost_best = ensemble.GradientBoostingRegressor(\n",
    "    n_estimators=model_cv.best_params_['n_estimators'],\n",
    "    max_depth=model_cv.best_params_['max_depth'],\n",
    "    min_samples_split=model_cv.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=model_cv.best_params_['min_samples_leaf'],\n",
    "    learning_rate=model_cv.best_params_['learning_rate'],\n",
    "    loss=model_cv.best_params_['loss']\n",
    "    )\n",
    "\n",
    "xgboost_best.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r2 score on test set: 0.2572\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(y_test, xgboost_best.predict(x_test))\n",
    "print(\"The r2 score on test set: {:.4f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/xgboost_model.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../models/xgboost_model.joblib'\n",
    "joblib.dump(xgboost_best, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_two(reg1, reg2, X, y, metric='default'):\n",
    "\n",
    "  # Choose seeds for each 2-fold iterations\n",
    "  seeds = [13, 51, 137, 24659, 347]\n",
    "\n",
    "  # Initialize the score difference for the 1st fold of the 1st iteration \n",
    "  p_1_1 = 0.0\n",
    "\n",
    "  # Initialize a place holder for the variance estimate\n",
    "  s_sqr = 0.0\n",
    "\n",
    "  # Initialize scores list for both classifiers\n",
    "  scores_1 = []\n",
    "  scores_2 = []\n",
    "  diff_scores = []\n",
    "\n",
    "  # Iterate through 5 2-fold CV\n",
    "  for i_s, seed in enumerate(seeds):\n",
    "\n",
    "    # Split the dataset in 2 parts with the current seed\n",
    "    folds = KFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Initialize score differences\n",
    "    p_i = np.zeros(2)\n",
    "\n",
    "    # Go through the current 2 fold\n",
    "    for i_f, (trn_idx, val_idx) in enumerate(folds.split(X)):\n",
    "      # Split the data\n",
    "      trn_x, trn_y = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "      val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "      # Train regression\n",
    "      reg1.fit(trn_x, trn_y)\n",
    "      reg2.fit(trn_x, trn_y)\n",
    "\n",
    "      # Compute scores\n",
    "      preds_1 = reg1.predict(val_x)\n",
    "      score_1 = r2_score(val_y, preds_1)\n",
    "      \n",
    "      preds_2 = reg2.predict(val_x)\n",
    "      score_2 = r2_score(val_y, preds_2)\n",
    "\n",
    "      if metric == \"adj_r2\":\n",
    "        score_1 = base_train_adj_r2 = get_adj_r2(\n",
    "          n_observations=len(trn_y) / 2,\n",
    "          n_independent_variables=trn_x.shape[1],\n",
    "          r2_score = score_1\n",
    "        )\n",
    "\n",
    "        score_2 = base_train_adj_r2 = get_adj_r2(\n",
    "          n_observations=len(trn_y) / 2,\n",
    "          n_independent_variables=trn_x.shape[1],\n",
    "          r2_score = score_2\n",
    "        )\n",
    "\n",
    "\n",
    "      # keep score history for mean and stdev calculation\n",
    "      scores_1.append(score_1)\n",
    "      scores_2.append(score_2)\n",
    "      diff_scores.append(score_1 - score_2)\n",
    "      print(\"Fold %2d score difference = %.6f\" % (i_f + 1, score_1 - score_2))\n",
    "\n",
    "      # Compute score difference for current fold  \n",
    "      p_i[i_f] = score_1 - score_2\n",
    "\n",
    "      # Keep the score difference of the 1st iteration and 1st fold\n",
    "      if (i_s == 0) & (i_f == 0):\n",
    "        p_1_1 = p_i[i_f]\n",
    "\n",
    "    # Compute mean of scores difference for the current 2-fold CV\n",
    "    p_i_bar = (p_i[0] + p_i[1]) / 2\n",
    "\n",
    "    # Compute the variance estimate for the current 2-fold CV\n",
    "    s_i_sqr = (p_i[0] - p_i_bar) ** 2 + (p_i[1] - p_i_bar) ** 2 \n",
    "\n",
    "    # Add up to the overall variance\n",
    "    s_sqr += s_i_sqr\n",
    "    \n",
    "  # Compute t value as the first difference divided by the square root of variance estimate\n",
    "  t_bar = p_1_1 / ((s_sqr / 5) ** .5) \n",
    "\n",
    "  print(\"Regression 1 mean score and stdev : %.6f + %.6f\" % (np.mean(scores_1), np.std(scores_1)))\n",
    "  print(\"Regression 2 mean score and stdev : %.6f + %.6f\" % (np.mean(scores_2), np.std(scores_2)))\n",
    "  print(\"Score difference mean + stdev : %.6f + %.6f\" \n",
    "        % (np.mean(diff_scores), np.std(diff_scores)))\n",
    "  print(\"t_value for the current test is %.6f\" % t_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 score difference = -0.006949\n",
      "Fold  2 score difference = -0.063067\n",
      "Fold  1 score difference = -0.141070\n",
      "Fold  2 score difference = -0.016634\n",
      "Fold  1 score difference = -0.125163\n",
      "Fold  2 score difference = -0.059041\n",
      "Fold  1 score difference = -0.031952\n",
      "Fold  2 score difference = -0.058188\n",
      "Fold  1 score difference = -0.038647\n",
      "Fold  2 score difference = -0.130422\n",
      "Regression 1 mean score and stdev : 0.131042 + 0.051056\n",
      "Regression 2 mean score and stdev : 0.198155 + 0.038910\n",
      "Score difference mean + stdev : -0.067113 + 0.046069\n",
      "t_value for the current test is -0.122626\n"
     ]
    }
   ],
   "source": [
    "five_two(\n",
    "    reg1=xgboost,\n",
    "    reg2=xgboost_best,\n",
    "    X=df_sol_X,\n",
    "    y=df_sol_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20444563,  0.26215949,  0.23695625,  0.3413003 ,  0.24612715,\n",
       "        0.26305958, -0.14225403,  0.15206445,  0.26732598,  0.20390257])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(estimator=xgboost_best, X=df_sol_X, y=df_sol_y, cv=folds, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10325867,  0.15040459,  0.17747865,  0.12050908,  0.20986007,\n",
       "        0.22875968, -0.30601322,  0.09060705,  0.13289577,  0.15118151])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=xgboost, X=df_sol_X, y=df_sol_y, cv=folds, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
