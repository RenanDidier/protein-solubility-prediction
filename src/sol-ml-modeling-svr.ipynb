{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit Learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Scipy libraries\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/\"\n",
    "\n",
    "data_path = folder_path + \"complex_processed_data.csv\"\n",
    "standardized_data_path = folder_path + 'complex_processed_standardized_data.csv'\n",
    "standardized_poutliers_removed_data_path = folder_path + 'complex_processed_standardized_outliers_removed_data.csv'\n",
    "\n",
    "df_solubility = pd.read_csv(standardized_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataset\n",
    "\n",
    "Process Dataset before the model creation.\n",
    "The following actions were done:\n",
    "* Split the independent variable from the dependent ones;\n",
    "* Split Dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into X and Y for machine learning\n",
    "\n",
    "df_sol_X = df_solubility.copy()\n",
    "df_sol_X.drop(columns=['solubility'], axis=1, inplace=True)\n",
    "\n",
    "df_sol_y = df_solubility[['solubility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                        df_sol_X, df_sol_y, \n",
    "                        train_size = 0.8,\n",
    "                        test_size = 0.2,\n",
    "                        random_state = 10\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def get_adj_r2(n_observations, n_independent_variables, r2_score):\n",
    "    Adj_r2 = 1 - ((1 - r2_score) * (n_observations - 1)) / (n_observations - n_independent_variables - 1)\n",
    "    return Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validation scheme to be used for train and test\n",
    "cv = 10\n",
    "folds = KFold(n_splits = cv, shuffle = True, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVR model with cross validation and default parameters\n",
    "svr = SVR(kernel='rbf', C=1.0, gamma='auto', epsilon=0.1, degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1600 candidates, totalling 16000 fits\n",
      "{'C': 1.5, 'degree': 1, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Specify range of hyperparameters to tune\n",
    "hyper_params = {\n",
    "    'kernel': ('linear', 'rbf','poly', 'sigmoid'),\n",
    "    'C':[1, 1.5, 5, 10, 100],\n",
    "    'gamma': [1e-7, 1e-4, 'auto', 'scale'],\n",
    "    'epsilon':[0.1,0.2,0.3,0.4,0.5],\n",
    "    'degree': [1,2,3,4]\n",
    "    }\n",
    "\n",
    "\n",
    "# Call GridSearchCV()\n",
    "model_cv = GridSearchCV(\n",
    "    estimator = SVR(),\n",
    "    param_grid = hyper_params,\n",
    "    scoring= 'r2',\n",
    "    cv = folds,\n",
    "    verbose = 1,\n",
    "    return_train_score=True,\n",
    "    n_jobs = -1,\n",
    "    refit = True\n",
    "    )\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "best_model = model_cv.fit(x_train, np.ravel(y_train)) \n",
    "\n",
    "print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model with best_params_ from grid search\n",
    "# Use cross validation on the best_params_ model\n",
    "\n",
    "svr_best = SVR(\n",
    "    kernel=model_cv.best_params_['kernel'],\n",
    "    C=model_cv.best_params_['C'],\n",
    "    gamma=model_cv.best_params_['gamma'],\n",
    "    epsilon=model_cv.best_params_['epsilon'],\n",
    "    degree=model_cv.best_params_['degree']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical hypothesis testing\n",
    "\n",
    "Validate if the grid model is better than the base model\n",
    "\n",
    "Null hyphotesis and Alternative hyphotesis\n",
    "* Ho = Best params R2 and Adj R2 <= base model R2 and Adj R2\n",
    "* Ha = Best params R2 and Adj R2 > base model R2 and Adj R2\n",
    "\n",
    "Errors:\n",
    "* Type I Error: false positive, reject the Ho but it is true\n",
    "* Type II Error: false negative, do not reject the Ho but its false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_two(reg1, reg2, X, y, metric='default'):\n",
    "\n",
    "  # Choose seeds for each 2-fold iterations\n",
    "  seeds = [13, 51, 137, 24659, 347]\n",
    "\n",
    "  # Initialize the score difference for the 1st fold of the 1st iteration \n",
    "  p_1_1 = 0.0\n",
    "\n",
    "  # Initialize a place holder for the variance estimate\n",
    "  s_sqr = 0.0\n",
    "\n",
    "  # Initialize scores list for both classifiers\n",
    "  scores_1 = []\n",
    "  scores_2 = []\n",
    "  diff_scores = []\n",
    "\n",
    "  # Iterate through 5 2-fold CV\n",
    "  for i_s, seed in enumerate(seeds):\n",
    "\n",
    "    # Split the dataset in 2 parts with the current seed\n",
    "    folds = KFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Initialize score differences\n",
    "    p_i = np.zeros(2)\n",
    "\n",
    "    # Go through the current 2 fold\n",
    "    for i_f, (trn_idx, val_idx) in enumerate(folds.split(X)):\n",
    "      # Split the data\n",
    "      trn_x, trn_y = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "      val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "      # Train regression\n",
    "      reg1.fit(trn_x, trn_y)\n",
    "      reg2.fit(trn_x, trn_y)\n",
    "\n",
    "      # Compute scores\n",
    "      preds_1 = reg1.predict(val_x)\n",
    "      score_1 = r2_score(val_y, preds_1)\n",
    "      \n",
    "      preds_2 = reg2.predict(val_x)\n",
    "      score_2 = r2_score(val_y, preds_2)\n",
    "\n",
    "      if metric == \"adj_r2\":\n",
    "        score_1 = base_train_adj_r2 = get_adj_r2(\n",
    "          n_observations=len(trn_y) / 2,\n",
    "          n_independent_variables=trn_x.shape[1],\n",
    "          r2_score = score_1\n",
    "        )\n",
    "\n",
    "        score_2 = base_train_adj_r2 = get_adj_r2(\n",
    "          n_observations=len(trn_y) / 2,\n",
    "          n_independent_variables=trn_x.shape[1],\n",
    "          r2_score = score_2\n",
    "        )\n",
    "\n",
    "\n",
    "      # keep score history for mean and stdev calculation\n",
    "      scores_1.append(score_1)\n",
    "      scores_2.append(score_2)\n",
    "      diff_scores.append(score_1 - score_2)\n",
    "      print(\"Fold %2d score difference = %.6f\" % (i_f + 1, score_1 - score_2))\n",
    "\n",
    "      # Compute score difference for current fold  \n",
    "      p_i[i_f] = score_1 - score_2\n",
    "\n",
    "      # Keep the score difference of the 1st iteration and 1st fold\n",
    "      if (i_s == 0) & (i_f == 0):\n",
    "        p_1_1 = p_i[i_f]\n",
    "\n",
    "    # Compute mean of scores difference for the current 2-fold CV\n",
    "    p_i_bar = (p_i[0] + p_i[1]) / 2\n",
    "\n",
    "    # Compute the variance estimate for the current 2-fold CV\n",
    "    s_i_sqr = (p_i[0] - p_i_bar) ** 2 + (p_i[1] - p_i_bar) ** 2 \n",
    "\n",
    "    # Add up to the overall variance\n",
    "    s_sqr += s_i_sqr\n",
    "    \n",
    "  # Compute t value as the first difference divided by the square root of variance estimate\n",
    "  t_bar = p_1_1 / ((s_sqr / 5) ** .5) \n",
    "\n",
    "  print(\"Regression 1 mean score and stdev : %.6f + %.6f\" % (np.mean(scores_1), np.std(scores_1)))\n",
    "  print(\"Regression 2 mean score and stdev : %.6f + %.6f\" % (np.mean(scores_2), np.std(scores_2)))\n",
    "  print(\"Score difference mean + stdev : %.6f + %.6f\" \n",
    "        % (np.mean(diff_scores), np.std(diff_scores)))\n",
    "  print(\"t_value for the current test is %.6f\" % t_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 score difference = -0.027373\n",
      "Fold  2 score difference = -0.034255\n",
      "Fold  1 score difference = 0.034320\n",
      "Fold  2 score difference = 0.029165\n",
      "Fold  1 score difference = -0.070850\n",
      "Fold  2 score difference = -0.081545\n",
      "Fold  1 score difference = -0.008678\n",
      "Fold  2 score difference = -0.058249\n",
      "Fold  1 score difference = 0.000059\n",
      "Fold  2 score difference = 0.017027\n",
      "Regression 1 mean score and stdev : 0.179571 + 0.030092\n",
      "Regression 2 mean score and stdev : 0.199609 + 0.039461\n",
      "Score difference mean + stdev : -0.020038 + 0.039196\n",
      "t_value for the current test is -1.598158\n"
     ]
    }
   ],
   "source": [
    "five_two(\n",
    "    reg1=svr,\n",
    "    reg2=svr_best,\n",
    "    X=df_sol_X,\n",
    "    y=df_sol_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 score difference = -0.034649\n",
      "Fold  2 score difference = -0.043304\n",
      "Fold  1 score difference = 0.043443\n",
      "Fold  2 score difference = 0.036869\n",
      "Fold  1 score difference = -0.089684\n",
      "Fold  2 score difference = -0.103085\n",
      "Fold  1 score difference = -0.010984\n",
      "Fold  2 score difference = -0.073636\n",
      "Fold  1 score difference = 0.000075\n",
      "Fold  2 score difference = 0.021524\n",
      "Regression 1 mean score and stdev : -0.037847 + 0.038478\n",
      "Regression 2 mean score and stdev : -0.012504 + 0.050316\n",
      "Score difference mean + stdev : -0.025343 + 0.049574\n",
      "t_value for the current test is -1.601135\n"
     ]
    }
   ],
   "source": [
    "five_two(\n",
    "    reg1=svr,\n",
    "    reg2=svr_best,\n",
    "    X=df_sol_X,\n",
    "    y=df_sol_y,\n",
    "    metric='adj_r2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t statistic: -0.634\n",
      "p value: 0.554\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=svr,\n",
    "                          estimator2=svr_best,\n",
    "                          X=df_sol_X, y=df_sol_y,\n",
    "                          scoring='r2',\n",
    "                          random_seed=42)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p value: %.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/svr_model.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../models/svr_model.joblib'\n",
    "joblib.dump(svr_best, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "What was done:\n",
    "* Split dataset in test 20% and train 80%;\n",
    "* Create a SVR model with default parameters and another with grid seach + cross validation;\n",
    "* Compare the test scores (r2 and adj r2) from before the grid search and after using T test using 5 x 2-fold cross validation (5 cv of 2 folds);\n",
    "* On grid search: C=1000 did not show better results so it was removed\n",
    "* It seems that the grid model is better than the basemodel with a 80% confidence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
