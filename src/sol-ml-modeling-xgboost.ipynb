{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit Learn libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Scipy libraries\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Utils functions\n",
    "from utils.utils import kfold, five_two, read_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = read_datasets(\n",
    "    'x_train.csv',\n",
    "    'x_test.csv',\n",
    "    'y_train.csv',\n",
    "    'y_test.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting regression - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validation scheme to be used for train and test\n",
    "folds = kfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15000 candidates, totalling 150000 fits\n",
      "[23:53:50] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/gbm/gbtree.cc:82: DANGER AHEAD: You have manually specified `updater` parameter. The `tree_method` parameter will be ignored. Incorrect sequence of updaters will produce undefined behavior. For common uses, we recommend using `tree_method` parameter instead.\n",
      "{'updater': 'grow_histmaker', 'tree_method': 'exact', 'subsample': 0.7, 'sampling_method': 'uniform', 'reg_lambda': 0.3, 'refresh_leaf': 0, 'objective': 'reg:squarederror', 'n_estimators': 50, 'min_child_weight': 3, 'max_leaves': 8, 'max_depth': 1, 'max_delta_step': 0, 'grow_policy': 'depthwise', 'gamma': 0.5, 'eval_metric': 'rmse', 'eta': 0.2, 'colsample_bytree': 1, 'colsample_bynode': 0.5, 'colsample_bylevel': 0.5, 'booster': 'gbtree', 'base_score': 0.5, 'alpha': 0.4}\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    'colsample_bytree': [0.1, 0.2, 0.3],\n",
    "    'n_estimators': [10, 20, 25, 50], #75, 100\n",
    "    'subsample': [0.5, 0.6, 0.7],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'booster': ['gbtree', 'dart'],\n",
    "    'objective': [\"reg:squarederror\"],\n",
    "    'eta': [0.1, 0.2, 0.3, 0.4, 0.9],\n",
    "    'gamma': [0, 0.3, 0.5],\n",
    "    'min_child_weight': [2, 3, 4],\n",
    "    'max_delta_step':[0, 1],\n",
    "    'reg_lambda': [0.3, 0.5, 0.7],\n",
    "    'alpha': [0.2, 0.3, 0.4],\n",
    "    'tree_method': ['approx', 'auto', 'exact', 'gpu_hist', 'hist'],\n",
    "    'max_leaves': [0, 1, 3, 5, 8],\n",
    "    'eval_metric': ['rmse'],\n",
    "    'base_score': [0.1, 0.3, 0.4, 0.5],\n",
    "    'grow_policy': ['depthwise'],\n",
    "    'refresh_leaf': [0,1],\n",
    "    'sampling_method': ['uniform'],\n",
    "    'colsample_bylevel': [0.1, 0.2, 0.5, 1],\n",
    "    'colsample_bynode': [0.1, 0.2, 0.5, 1],\n",
    "    'colsample_bytree': [0.1, 0.2, 0.4, 1],\n",
    "    'updater': ['grow_colmaker', 'grow_histmaker', 'grow_quantile_histmaker', 'grow_gpu_hist', 'sync', 'refresh', 'prune']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call RandomizedSearchCV()\n",
    "model_cv = RandomizedSearchCV(\n",
    "    estimator = xgb.XGBRegressor(),\n",
    "    param_distributions = hyper_params,\n",
    "    n_iter=15000,\n",
    "    scoring= 'r2',\n",
    "    cv = folds,\n",
    "    verbose = 2,\n",
    "    return_train_score=True,\n",
    "    n_jobs = -1,\n",
    "    refit = True\n",
    "    )\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "best_model = model_cv.fit(x_train, np.ravel(y_train)) \n",
    "\n",
    "print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model with best_params_ from grid search\n",
    "\n",
    "xgboost_best = best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Results: [0.3713780621407403, 0.36798204348901953, 0.35423043336248083, 0.35625547914772504, 0.37367218682338166, 0.3996905969800387, 0.3726975372339837, 0.3766437718662281, 0.3738449039230266, 0.3586751501868487]\n",
      "Test Results: [0.23531106489913378, 0.17146703312728806, 0.2995418880574914, 0.2906989877096088, 0.24810060401652845, -0.12393422762110728, 0.17751238491874788, 0.24623741719377834, 0.13510803796147353, 0.3306688216210407]\n"
     ]
    }
   ],
   "source": [
    "# Get the results for each split\n",
    "\n",
    "def get_best_model_cv_split_results(best_model, n_splits=10, set_type='train'):\n",
    "    results = []\n",
    "    for i in range(0, n_splits):\n",
    "        current_split = 'split{}_{}_score'.format(i, set_type)\n",
    "        split_result = best_model.cv_results_[current_split][best_index]\n",
    "        results.append(split_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Train Results: {}\".format(get_best_model_cv_split_results(best_model, 10, 'train')))\n",
    "print(\"Test Results: {}\".format(get_best_model_cv_split_results(best_model, 10, 'test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean: 0.3705070165153473\n",
      "Test mean: 0.20107120118839833\n"
     ]
    }
   ],
   "source": [
    "#Get the mean for the train and test\n",
    "\n",
    "train_mean = sum(get_best_model_cv_split_results(best_model, 10, 'train'))/10\n",
    "test_mean = sum(get_best_model_cv_split_results(best_model, 10, 'test'))/10\n",
    "\n",
    "print(\"Train mean: {}\".format(train_mean))\n",
    "print(\"Test mean: {}\".format(test_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r2 score on test set: 0.2677\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(y_test, xgboost_best.predict(x_test))\n",
    "print(\"The r2 score on test set: {:.4f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/xgboost_model.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../models/xgboost_model.joblib'\n",
    "joblib.dump(xgboost_best, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* Create 10 folds\n",
    "* Set grid for the RandomSearchCV\n",
    "* Search and fit the model with best params\n",
    "* Get the results (r2) for each train and test results for training data\n",
    "* Get the result (r2) for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
